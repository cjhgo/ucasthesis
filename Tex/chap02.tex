\chapter{相关工作介绍}
本章对分布式图计算系统中的相关概念和研究工作进行介绍。
在2.1节中本论文对现有的分布式图计算系统分别按照任务调度机制，通信机制，图划分，计算粒度及顶点计算模型这四个方面进行了一般性综述。
在2.2节中本论文对LazyGraph 这个分布式图计算系统和它提出的延迟数据一致性方法(LazyAsync)进行了重点介绍。
LazyGraph是本论文所使用的分布式图计算系统。
LazyAsync是本论文提出的自适应优化方法这一工作的基础。
\section{分布式并行图处理框架}
% 由于MapReduce编程范式存在着进行多轮迭代图计算时带来中间结果存储开销，实现图算法需要额外转换等问题，
% 人们在并行分布式处理大规模图数据过程中广泛使用BSP(Bulk Synchronous Parallel)模型\cite{bsp@1990}。
在对大规模图数据进行分布式并行处理时，BSP(Bulk Synchronous Parallel)模型\cite{bsp@1990}比MapReduce更为合适。
BSP模型由图灵奖得主Valiant于1990年提出，
它是一种基于消息通信的并行计算模型。
它的核心思想是将一个巨大复杂的计算任务分解为一系列的迭代运算，因此尤其适合做数据的迭代计算。
如图\ref{fig:bsp}所示，一个BSP作业由若干个顺序执行的超步（super step)组成：$S_1,S_2,\cdots,S_n$，对应于$n$次迭代处理。
并行计算的任务按照超步进行组织，在超步$S_i$内，各任务异步接受来自$S_{i-1}$的消息，执行本地计算并发送消息给下一个超步$S_{i+1}$。
BSP模型对相邻的超步进行同步控制，确保超步$S_i$内的所有任务均已完成才进入下一个超步$S_{i+1}$，这种同步方式可避免死锁和数据竞争问题。
与MapReduce相比，BSP这样的运行逻辑避免了由于多次迭代产生的数据反复迁移和作业连续调度的问题，从而可以使得迭代处理更加灵活，
更加适合处理图计算问题。
\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.40\textwidth]{bsp}
  \bicaption{BSP模型}{bsp model}
  \label{fig:bsp}
\end{figure}

受BSP计算模型的启发，Google在2010年首次提出了以顶点为中心的分布式图计算框架Pregel\cite{Malewicz@SIGMOD10}。
自Pregel之后，围绕着Pregel存在的局限性，以及新的问题和需求，人们又开发出了一系列新的用于处理大规模图数据的分布式并行计算框架。
这些框架分别在任务调度机制，通信机制，图划分，计算粒度与顶点计算模型这四个方面上存在着不同\cite{TLV, reviewruc}。

\subsection{任务调度机制}
任务调度主要解决如何按照一定的规则对计算资源进行分配的问题。
在分布式图计算中，为了确保负载均衡避免木桶效应，任务调度问题必须得到有效的解决。
根据迭代计算过程系统中是否有多个超步在同时运行，
分布式图计算框架采用的任务调度机制可以分为同步调度，异步调度和混合调度这三类。

同步调度模式，指对两个相邻的超步$S_i$与$S_{i+1}$，必须在$S_i$超步结束之后才能够进入下一个$S_{i+1}$超步，
任何时刻系统中不存在两个交错运行的超步。
这种模式之下，下一个超步$S_{i+1}$执行运算时所看到的数据必然是上一个超步$S_i$的所有顶点执行完成之后的结果数据。
这种调度模式由于控制机制比较简单，因而便于用户理解及操作。
利用同步模式的分布式图计算框架运行图算法时，由于运行的过程直观明了，运行结果具有确定性，
因而有助于程序的设计，调试。
然而，这种调度模式下，由于在处理相同的超步$S_i$时，
不同计算顶点的处理能力不同，同时处理的图数据划分不均匀，
因而各个顶点执行计算所需的时间存在显著差异，从而产生木桶效应，
导致运行总时间过长，造成计算资源的浪费。
同时，由于在这种模式下，数据的传输只能发生在两次相邻的超步$S_i$和$S_{i+1}$之间，
有一定可能会造成在某些运行场景下运行结果始终无法收敛。
为解决同步调度模式中的这些问题，研究人员进一步提出了异步调度模式和混合调度模式


在异步调度模式中，前后两次超步$S_i$和$S_{i+1}$之间没有明确的界限，
前一步$S_i$中较为活跃的图顶点只要接收到其需要的消息，不必等待其他所有图顶点完成，
便可以自由地接受调度器的调度执行下一步操作，并向其他图顶点广播其完成的结果数据。
这一模式摒弃了数据传输和执行只能在相邻两超步$S_i$与$S_{i+1}$之间的同步路障（barrier），
因而能够有效降低木桶效应，提供图计算的运行效率，同时解决某些算法在同步模式下无法收敛的问题。
然而，基于这一模式的分布式图计算框架在设计上较基于同步调度模式的分布式图计算框架要复杂许多，
系统设计时的编程难度也相应增加，需要设计出合适且高效的调度器。
同时在运行过程中可能出现异步计算过程中对数据的读写访问冲突，
计算过程和结果不确定等问题，运行需花费更多的精力反复就行修改调试。

在深入分析比较同步与异步调度模式之后，
许多分布式图计算框架中，将两者结合起来的混合调度模式被提出。
不同的混合调度模式实际上针对不同顶点和不同计算阶段分别切换同步和异步调度模式。
PowerSwitch\cite{Xie@PPoPP15}是比较有代表性的采用混合调度模式的系统。
它能够通过一组启发式算法建立一个代价收益模型，动态预测两种调度模式的性能，
并自由切换为性能最佳的调度模式，从而提高图计算的执行效率。

\subsection{通信机制}

通信机制，即分布式图计算框架中各个计算结点之间信息交流的方式。
当信息量过大时，信息传递的途径对分布式图计算系统的运行速度有着重要的影响。
分布式图计算框架目前常用的通信方式主要有基于共享内存和基于消息传递两大类。

基于共享内存通信的方式构建的分布式图计算框架中，
每个图顶点上的数据以共享变量的方式存储在计算机节点中，
其他图顶点可以通过框架提供的数据结构按照内存地址来快速读取。
然而在实际的应用中，
由于每个计算结点都有自己独立的内存地址空间且需要保证数据的一致性，
所以基于共享内存的通信机制实现起来较为困难。

基于消息传递的通信方式在实际上更为容易。
比较常用的网络通信协议有
RPC(remote procedure call)\cite{rpc}、Netty\cite{netty}、ActivMQ\cite{activemq}以及基于 MPI(message Passsing interface)\cite{mpi}
这几类。
在基于消息传递的通信机制中，
图顶点之间的信息交流通过网络通信平台在计算机节点间传送包含数据和目标顶点的ID的消息。
在这种方式中，系统会使用批量发送的方式来优化网络通信。
具体的方式为：若目标图顶点与发送消息的源顶点位于同一台机器上，
则直接将信息发送给相应的图顶点；
否则，消息将置于系统的消息发送缓冲池中，等待集中批量发送。
消息传递中的缓冲批量发送虽然能提供通信负载但同时也会因为消息的滞后而影响图计算的性能。


通信开销会严重影响整个分布式图处理系统的性能，
因此如何对基于消息传递或基于共享内存的通信方式进行优化
成为分布式图计算系统性能研究的重要任务。
现如今主要的优化方法主要从减少消息数量的角度进行入手：
一种是将图进行合理分割，降低图分区间的连通性，
从而降低跨机器的通信请求，从根本上减少跨机器的消息数量；
一种是通过将发送到相同目的顶点的消息合并成一条，
或者合并发送到相同机器的消息，从而减少消息的数量。


在系统进行消息传递通信的过程中，根据信息的流向，分布式图计算框架的工作模式可以划分为 push 模式和 pull 模式两种类型。

在 push 模式中，图中的活跃顶点完成计算产生数据信息后，随即发送给即将被激活的邻居顶点。
这种模式在内存资源充足的情况下可以减少数据传送时间，从而提高图计算效率。
然而在数据量较大的情况下，过多的数据信息存储会增加系统内存的压力。
而在pull模式下，只有图顶点需要某数据信息时才会主动向其他顶点请求数据，
从而避免了存储大量冗余的信息，减少了系统内存的压力。
这种模式下需要采取方法减少跨机器的消息数量，减少额外的通信开销。

两种模式各有利弊，分别适合不同的场景。
有不少研究工作通过混合使用两种模式来进一步提高图计算的执行效率。
单机图计算系统Ligra\cite{Shun@PPoPP13}混合采用push 和 pull 两种模式，
根据需要参与计算的活跃点的边数在两种模式间自适应地切换。
Gemini\cite{Zhu@OSDI16} 则将
在分布式环境中实现了这种双模式计算引擎，
并且进一步将两种模式下的计算过程都细分成发送端和接收端两个部分，
从而将分布式系统的通信从计算中剥离出来。

\subsection{图划分}

图数据划分是进行分布式图计算的基础，划分的合适与否直接影响着分布式图计算框架的性能。
在图划分过程中有两个要遵循的重要原则:一个是降低划分后子图之间的连通性，从而降低网络开销;
另一个是保证子图大小均匀，以实现系统的负载均衡\cite{reviewruc}。
图计算框架中的图划分技术可以从\textit{图划分方式和图划分策略}两个方面进行论述。

在分布式图处理系统中主要有两种图划分方式:
边切分方式(vertex-cut)\cite{Gonzalez@OSDI12}和点切分方式(edge-cut) \cite{Malewicz@SIGMOD10}。
边切分对顶点之间的边进行切分，即划分过程是把顶点分配到不同的计算结点上，然后在计算结点的子图内
把顶点之间的边补上去，划分后，被切断的边将出现在两个不同的子图中。
点切分是对图的节点进行切分，即划分过程是把边分配到不同的计算结点上，
划分后，每条边出现且仅出现在一个子图中，邻居多的点将会出现在多个子图中。

分布式图计算处理系统的图划分策略主要有 3 种: 
离线划分策略\cite{metis}、流式划分策略\cite{tsourakakis2014fennel}以及动态重划分策略\cite{kumar2017graphsteal} 。
离线划分策略在图数据被分布式图计算框架加载之前便将其划分为若干子图。
流式划分策略指在数据加载过程中对图数据边加载边划分。
这种策略假定数据以节点流或者边流的方式加载，根据已经到达数据的分布信息，建立一组启发式的规则，
从而决定已到达图数据的划分。
动态重划分策略首先收集系统运行过程中的状态数据，然后根据收集到的状态信息建立相应的模型，
指定划分策略，最后根据制定的策略进行划分。

\subsection{计算粒度与顶点计算模型}

粒度（Granularity）指在求解问题时研究对象的细致程度。
从不同的粒度对问题进行分析，有助于从不同角度对问题进行求解。
目前主要的分布式图计算处理系统分别以顶点、以子图或以路径作为计算粒度对图数据进行处理。
    
在以顶点为计算粒度的分布式图处理系统中，
迭代处理过程主要对图顶点进行遍历。
这种模式由于逻辑简单且具有良好的算法表达能力，
因而被绝大多数框架如GraphLab\cite{Low@12}、GraphX\cite{Gonzalez@OSDI14}、Giraph\cite{Avery@HS11}等采用。
不过该模式牺牲了图顶点访问子图内其他顶点信息的灵活性，因此存着着收敛速度过慢，网络通信量大等缺点。

针对以顶点为计算粒度模式的缺点，人们又提出了大量以子图为计算粒度的分布式图处理系统，
如GraphHP\cite{su2016graphhp}，GoFFish\cite{goffish}等。
这种模式先对子图的边界点进行处理，然后接着处理子图内部的节点。
%不过这种模式表达能力较弱，因此不能够取代以顶点为粒度的模式。

以路径为计算粒度的模式主要在单机集中式图处理系统中使用，这种模式将边数据和点数据进行分区，
对边采用顺序访问的方式进行遍历计算，减少了对边数据的随机访问，进而降低了磁盘访问开销。
目前为止，在分布式环境下以路径为计算粒度的模式还缺少有效实践，不过这种模式可以为分布式图计算框架中的某些优化提供借鉴意义。

绝大多数分布式图处理系统以顶点为计算粒度，用户需要以图中的顶点为粒度定义运算函数。
在实现运算函数的时候，人们采用的是“Think Like A Vertex”\cite{TLV}这种编程思想。
% 这个运算函数的功能包括消息的收发、节点值的更新以及顶点状态
% 的改变等。然后，系统按照该运算函数对图中的每一个顶点进行同步或者异步的更新运算。      
这种编程思想以顶点为中心，从顶点的角度出发去描述图计算过程中每个顶点的行为。
在编程框架的迭代过程中，图中所有顶点不断地执行用户定义的顶点程序(vertex program)，每个顶点只能从邻居顶点发送或接收消息，
当顶点收到消息后就处于激活状态，图计算的过程就是维护和更新每个顶点的状态的过程，当所有顶点都已经处于非激活状态，系统结束计算过程。
在具体实现过程中，计算框架主要把顶点程序分解为一个，两个或三个执行阶段。

单执行阶段的顶点程序可以用一个计算函数来表示，这个计算函数遵循这样的执行流程：访问邻居数据和消息，计算新的顶点状态值，分发状态变化值。
Pregel是采用了这种顶点程序抽象的一个典型代表。
两阶段的顶点程序常被分解为 Scatter-Gather 两个函数，并被称之为 Scatter-Gather 模型，
其中scatter阶段用于把顶点值分发给顶点，gather阶段用于收集输入然后用于执行顶点更新，
GraphChi\cite{GraphChi}使用Scatter-Gather作为标准的编程模型。
此外GRE\cite{GRE}编程框架也提出了一种在消息传递中使用的两阶段编程模型，Scatter-Combine，这种方法在活跃消息中既传递数据也传递操作。

PowerGraph\cite{Gonzalez@OSDI12}提出了一种三阶段执行过程的顶点程序，Gather-Apply-Scatter (GAS)。
Gather阶段对所有输入顶点或边上的消息进行求和，这个结果在Apply阶段用于更新顶点的状态，Scatter阶段把更新的消息传播出去。
PowerGraph是由共享内存架构的GraphLab\cite{Low@12}发展而来的，它既支持基于BSP实现的同步引擎，也支持采用灵活调度策略的异步引擎。
PowerGraph通过GAS的顶点编程模型配合vertex-cut的图划分方法，提高了图并行处理的粒度，较好地解决了图的幂律分布带来的计算不平衡问题。
在PowerGraph的vertex-cut划分方法中，图按边划分，每条边被唯一地分配到某个机器上去，顶点则在机器之间存在副本。
迭代过程中，每个顶点的副本能够互相独立的执行本地的gather阶段，然后各个副本把执行结果发送到指定好的master顶点上，
master顶点使用这个结果来执行完Apply阶段后把结果再发送给各个副本，然后各个副本又可以独立地执行本地的scatter阶段。


\begin{figure}[!htbp]
\centering
\includegraphics[width=0.40\textwidth]{gas_user}
\bicaption{用户视图下的GAS模型}{GAS Model in User View}
\end{figure}

在GAS的编程模型中，副本点的存在使得分布式图计算框架可以对单个顶点进行并行处理，并使不同机器上的邻居顶点在本地互相访问数据成为可能。
但是副本点也带来了数据一致性和副本点之间的消息传递问题，这些问题会在迭代计算过程中带来频繁的全局同步和通信开销，
LazyGraph\cite{Wang@PPoPP18}提出了一种 延迟数据一致性方法（LazyAsync），它基于
延迟数据一致性(lazy data coherency)的想法，将一个顶点的不同副本当作是独立的顶点，
它们可以维护和更新自己的数据，各自独立地进行本地计算，系统会选择合适的时间节点在副本点之间进行全局消息交换然后通过计算来使得它们达到数据一致，
这种方法消除了顶点计算过程中用于维护副本点之间的数据一致性所引起的各种消息传递的开销，减少了副本点之间消息交换的次数，极大地减少了系统中全局同步的次数和网络通信量。
副本点之间进行全局消息交换的时间节点直接影响着系统的性能，LazyGraph使用机器学习模型训练出的参数选择具体的时间节点。

% 自Pregel之后，研究者们使用各种优化手段提出了众多的分布式图计算框架。
% 这些框架大多数都对同一个顶点的不同副本点采取 eager datacoherency 的方式，因此带来频繁的全局同步和通信。 
% LazyGraph 采用 lazy data coherency 的方法来解决这个问题。
% Hieroglyph\cite{ju2017hieroglyph}也同样使得同一顶点的不同的副本点可以独立本地更新自己的数据。
% 但是，Hieroglyph关注于将计算从通信中解耦出来，以获得充足的本地计算，并且需要用户定义模块来解决数据一致性的问题。
% 而 LazyGraph 则关注于延迟副本点间的数据一致性，并同时减少全局同步次数和网络通信，并且在编程框架内部自动地维护副本点之间的数据一致性。
% 因此现有的研究不能直接用于处理我们提出的延迟数据一致性方法中遗留的几个问题。

\section{基于延迟数据一致性方法的 LazyGraph 图计算系统}
\subsection{顶点副本的延迟数据一致性方法}
% 延迟数据一致方法是什么

% 副本点
副本点在分布式并行图计算框架中发挥了巨大作用。
一方面它可以使得集群的多台机器对一个顶点进行并行处理，提高了计算效率。
另一方面，副本点使得顶点能够在通过本地内存直接访问那些被划分在远程机器的顶点，减少了通信开销。
图\ref{fig:graph_cut}给出了一个图在三台机器上分别进行点划分和边划分的结果。
可以看到图中的顶点划分之后产生了副本点。
以边划分为例，图\ref{fig:edge_cut}中C顶点在2号机器上有一个副本，
这使得系统可以对C顶点的两条边C-B和C-D进行并行处理。
此外，由于C顶点在2号机器上的副本点的存在，2号机器上的B顶点在访问它的邻居C顶点时可以从本机内存中直接访问，
而不必通过网络请求从3号机器上访问。



\begin{figure}[!htbp]
  \centering
  \begin{subfigure}[b]{0.35\textwidth}
    \includegraphics[width=\textwidth]{vertex_cut}
    \caption{}
    \label{fig:vertex_cut}
  \end{subfigure}%
  ~% add desired spacing
  \begin{subfigure}[b]{0.35\textwidth}
    \includegraphics[width=\textwidth]{edge_cut}
    \caption{}
    \label{fig:edge_cut}
  \end{subfigure}
\bicaption{点划分和边划分}{vertex-cut and edge-cut}
\label{fig:graph_cut}
\end{figure}

% eager data coherence 及其 冗余

副本点在带来巨大好处的同时也引入了数据一致性的问题。
对于一个顶点在集群中同时分布存在的多个副本，之前的多数分布式图计算编程框架都采用 急切数据一致性（eager data coherency） 的方式来维护
这些副本之间的数据一致。
这种方式要求对顶点的任何修改都要在下一次新的修改之前同步到其他所有副本上。
图\ref{fig:pg-eager}展示了典型的图计算框架PowerGraph是如何实现 eager data coherency 的。
在PowerGraph的实现方式中，顶点的多个副本中某一个被标记为 master 点。对顶点的修改只在 master 点上进行，
修改之后把master 点的最新状态通过网络同步拷贝到其他副本点上。

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.40\textwidth]{pg-eager}
  \bicaption{PowerGraph 的 eager data coherency 实现\cite{Wang@PPoPP18}}{Eager Data Coherency in PowerGraph}
  \label{fig:pg-eager}
\end{figure}

eager data coherency 方式虽然最大程度地维护了顶点上数据的一致性，但是却带来了大量的冗余。
由于副本点上的数据必须从 master 上拷贝而来，因此在 master 上的数据进行修改时副本点上无法进行计算，
这带来了冗余的等待。
同时，由于master点上的数据必须通过网络同步拷贝给其他机器上的副本点，系统中因此引入了大量频繁的全局同步和通信。

% lazy data coherence 的方法
针对 eager data coherency 存在的问题，我们提出了一种称之为 LazyAsync 的延迟数据一致性方法，
并通过对通信机制和图加载划分等方面的其他工作把这个方法完善为了一个图计算系统LazyGraph\cite{Wang@PPoPP18}。
这种方法在保留副本点带来的好处的同时避免了 eager data coherency 方法存在的各种冗余问题。
在 LazyAsync 方法中，每个副本点都被当作一个独立的顶点，并且各自可以拥有自己的本地数据。
顶点在不同机器上的副本点分别维护着自己不同的本地视图，并且基于公式
$x_{i}^{t+1}=x_{i}^{t}+_{o p} \oplus_{j \rightarrow i \in E} \Delta_{j}^{t}$
进行迭代更新。
虽然不同的副本点它们收到的消息的次序不一致，但是只要有相同的初值和消息和，它们最终能够得到一致的值\cite{zlj2018}。

LazyAsync 将副本点 $v_1 , v_2 , \cdots , v_k$ 的计算过程分为本地计算和数据一致性两个阶段。
其仅仅在数据一致性阶段将各自的本地消息和其它副本点的远程消息进行合并，再通过计算实现数据一致。
因此，LasyAsync 的模式使得副本点在相邻两个数据一致性点之间维护各自不同的本地视图，
并在数据一致性阶段通过计算得到一致的全局视图。


LazyGraph 保留了现存分布式图计算编程框架的编程接口，同样使用GAS模式，
但是，需要用户将原来顶点程序改为 push-style 模式并支持 deltaMsg 消息传递\cite{zlj2018}。
和现存的分布式图计算编程框架一样，在有向图$G = \{V, E\}$上用户定义一个顶点程序 P，然后图中所有顶点$v \in V$并行地在多台机器上进行计算。
顶点之间的通信直接通过给对方发送消息来完成。一个顶点可以接受其它点发送过来的消息，然后修改自己的数据，再发送消息给其它顶点。
而图中的边用来传输数据。


当然，LazyAsync 也和现存的分布式图计算编程框架存在区别。
LazyAsync 要求顶点的迭代计算是可以用公式 $x_i^{t+1} = x_i^t +_{op} \oplus_{j \rightarrow i \in E} \Delta_j^t$ 来表示，
其中，$i$为顶点的 id 号，$t$为迭代计数器，$+_{op}$和$\oplus$表示运算符，$\Delta_j^t$表示顶点$j$在第t次迭代的消息变化值（deltaMsg）。
此外，$\oplus$是由用户定义的消息累加的运算符，LazyAsync 要求它必须是可交换且可累加的。
在 Gather 阶段，顶点$i$收到邻居点发送给它的消息$\Delta_j^t$，然后用$\oplus$操作进行累加求和，得到累加和 accum。
然后在 Apply 阶段，中心点用自己的数据和刚刚求得的累加和 accum 进行迭代更新$x_i^{t+1} = x_i^t +_{op} accum$。
最后，在 Scatter 阶段，点$i$的变化值$\Delta_i^{t+1}$又通过边发送给其它相邻的顶点。


虽然在用户的视图下，其定义的顶点程序$P$定义在图$G = \{V,E\}$中的顶点上，
但是在分布式的编程框架中，$P$ 是运行在每台机器上被划分后的子图上的。
在图的加载到分布式环境的过程中，LazyAsync 使用点划分的形式将顶点划分开，若干的顶点产生副本点并横跨在多台机器上。
运行时系统能将用户定义的顶点程序 P 转变为更低层次的图操作，再使用 LazyAsync 来并行异步地执行顶点的计算。

在以下的内容中，我们将介绍如何用 LazyAsync 去并行异步地进行顶点的计算。
LazyAsync 重新定义了在运行时系统中，副本点$v_0 , v_1 , ... , v_k$如何进行计算，并得到顶点的值。
如图\ref{fig:lazy_data_coherency}所示，
LazyAsync 将在分布式环境下副本点$v_0 , v_1 , ... , v_k$的计算分为本地计算阶段（local computation statges）和数据一致性阶段（data coherency stages）。
在本地计算阶段中，同一个顶点的副本点维护着各自不同的本地视图，然后用在同一台机器上从本地的邻居顶点收到的本地消息更新自己的数据。
顶点的新的本地视图对于本地的邻居顶点而言是立即可见的，而不用等待数据一致性点。
在本地计算的同时，所有的副本点都会累加从本地邻居点发送过来的变化信息。
而在数据一致性阶段，每个副本点都会把自己的消息变化的累加和通过网络发送给远程的其它副本点，并接收其它副本点发送过来的消息累加和。
之后，所有接收到消息的副本点都在自己原本的本地视图和其它副本点发送过来的消息和执行 Apply 操作，最终达到一致的全局视图。
\begin{figure}[!htbp]
\centering
\includegraphics[height=12cm]{lazy_data_coherency.png}
\bicaption{点$v$的延迟数据一致性过程\cite{zlj2018}}{ lazy data coherency for replicas of v}
\label{fig:lazy_data_coherency}
\end{figure}

\subsection{延迟数据一致性方法的开启策略}
% 现有的开启策略，及不足之处

数据一致性节点的选择，也就是 延迟数据一致性方法（LazyAsync）的开启策略， 是 LazyAsync 的重要组成部分。
% 延迟数据一致性方法要解决的一个重要问题就是它的开启策略。
在把迭代公式修改为差值累加的形式之后， LazyAsync 方法正是通过lazy data coherency 的方法来减少冗余的通信和同步。
LazyAsync 的开启策略指的是系统应该在何时对副本点开启 lazy data coherency，
让各副本点独立计算数轮之后再进行消息交换，以及lazy data coherency 持续多久。
一种策略是立即进入策略，即系统一开始运行就启用 lazy data coherency。
另一种策略是手动调优策略，即不断的手动指定启用 lazy data coherency 的全局超步迭代轮次，
然后取其中性能提升最好的一个结果。
手动调优策略虽然不实用但是能够得出LazyAsync在指定算法和输入图上的相对最优的性能提升效果。
在实验中我们发现，随着这些具体策略的不同，系统得到的加速比也不同。
在有些算法和输入图上立即进入策略就能得到相对最好的性能提升效果，
在有些算法和输入图上需要手动调优才能得到相对最好的性能提升效果。
显然这个策略既不是越早越好，也不是越晚越好，过早开启或者过完开启都有可能得不到相对最优的性能提升。

在之前的工作中，我们采用了一个Input-behavior-interval决策树模型来作为LazyAsync的开启策略。
如图\ref{fig:dtree}所示，这个决策树模型采用两个特征作为判断条件：

\begin{enumerate}
  \item 输入图的本地性（locality）。我们使用边和顶点的比例 $\frac{E}{V}$ 和复制因子 $\lambda$ 来表示输入图的本地性。
  其中，复制因子 $\lambda$ 定义为整个图中所有顶点的平均副本点数，也就是总的副本点数于总的顶点数的比值。
  \item 图算法的运行时特征。大多数图算法都是迭代式地进行计算，不断地更新顶点的数据直至达到某个特定的收敛条件。
  在每一轮的迭代中，顶点的活跃点数\verb|vcnt| 是不断变化的。
  我们使用活跃顶点数的变化趋势来描述算法的特征。
  具体的，用如下公式进行计算：
  \begin{equation}
  \label{equ:chap04:trend}
  trend = \frac{vcnt^{t-1}-vcnt^t}{vcnt^{t-1}}
  \end{equation}
  我们在数据一致性阶段计算每一轮迭代时的活跃顶点数的下降趋势 \verb|trend|，如果 \verb|trend| 是负值，那么算法处于一个上升阶段，否则，则处于下降阶段。
\end{enumerate}

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.40\textwidth]{decision_tree}
\bicaption{决策树模型\cite{zlj2018}}{Dceieson Tree Model}
\label{fig:dtree}
\end{figure}


Input-behavior-interval 决策树模型是一个离线调优模型。
它的基本思路是先通过手动调优找到各种情况组合下的相对最优的LazyAsync的开启策略。
然后对这个过程中产生的大量数据集进行训练拟合。
最后把拟合得到的决策树模型反代入到系统实现中作为一个在线的判断策略。


Input-behavior-interval 决策树模型并不能真正有效地解决 LazyAsync 的开启策略问题。
一方面在 Input-behavior-interval 决策树模型的训练过程中需要手动选择各种特征点组合进行试错。
另一方为了得到训练数量数据，也需要事先在大量的算法和输入图组合上进行繁琐的手动调优。
最重要的是基于机器学习方法得到的优化方法存着固有的过拟合或泛化能力不足的问题。
实际的计算过程中如果遇到训练集和验证集中没有遇到过的例子，决策树模型指导下的LazyAsync可能无法得到相对最优的性能提升。
图\ref{fig:dtfail}就给出了这样一个例子。
图\ref{fig:dtfail}对比了在不同集群配置环境下，同一算法分别采用基于eager data coherency的同步引擎，
基于决策树开启策略的LazyAsync，基于手动调优策略的LazyAsync三种方法进行计算所用的运行时间。
从图中可以看到在8机，24机，48机的集群环境中，决策树策略的运行时间和手动调优相一致，
可是在16机，32机，40机的集群环境中，决策树策略的运行时间与同步引擎的运行时间一致甚至更长。
也就是说在16机，32机，40机的集群环境中，决策树模型指导下的LazyAsync没有得到性能提升，出现了失灵的情况。

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.40\textwidth]{dtreefail}
\bicaption{决策树失灵的情况}{Failure of decision tree}
\label{fig:dtfail}
\end{figure}

LazyAsync对副本上的数据进行lazy data coherency，
已经被证明为是一种行之有效的提升图计算执行效率的方法。
但是关于这种方法如何得到相对最优的性能提升这一问题还有待研究。
我们之前的工作中采用决策树的方法来解决 LazyAsync 的优化问题，
但是决策树模型一方面自身的训练需要一个繁琐的手动调优过程，另一方面也存在着失灵的情况，
因此并不能真正有效的解决这一问题。


自 Pregel 之后，研究者们使用各种优化手段提出了众多的分布式图计算框架。
这些框架大多数都对同一个顶点的不同副本点采取 eager data coherency 的方式。
在相关工作中，Hieroglyph\cite{ju2017hieroglyph}也同样使得同一顶点的不同的副本点可以独立本地更新自己的数据。
但是，Hieroglyph 关注于将计算从通信中解耦出来， 以获得充足的本地计算，并且需要用户定义模块来解决数据一致性的问题。
而 LazyAsync 则关注于延迟副本点间的数据一致性， 并同时减少全局同步 次数和网络通信，并且在编程框架内部自动地维护副本点之间的数据一致性。 
因此现有的研究不能直接用于处理我们提出的LazyAsync中遗留的优化问题。


本文通过进一步的实验，提出了一种基于解的局部性的自适应优化方法有效地解决了 LazyAsync 方法的优化问题。
本文先是通过实验和理论分析，解释了LazyAsync 方法在不同的开启策略下得到不同程度的性能提升的现象。
并从有效计算的角度出发，论证了 LazyAsync 在避免了 eager data coherency 的冗余通信和同步的同时，
自身却有可能带来一定程度的冗余计算，因而无法得到相对最优的性能提升。

通过分析全局解和局部解的关系，我们进一步发现了图计算过程中解的局部性规律。
解的局部性规律可以用于指导LazyAsync减少冗余计算。
基于这个规律，本文最终提出了一种基于解的局部性的在线自适应优化方法。
这种方法不需要事先的离线训练，同时也能正确处理决策树模型失灵的情况，因而有效地解决了
LazyAsync 方法的优化问题。

\section{本章小结}
在这一章中，本论文先从任务调度机制，通信机制，图划分，计算粒度及顶点计算模型这些角度
对当前分布式图计算系统的研究进展进行了介绍。
接下来本论文着重介绍了LazyGraph这一分布式图计算系统，尤其是它所提出的延迟数据一致性方法和这一方法的开启策略。
这一方法的现有开启策略存在不足，本论文对此进一步展开研究，提出了一种基于解的局部性的自适应优化方法。